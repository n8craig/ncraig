---
title: "GDELT Summary Explorer API Calls in R"
description: |
  This post looks at submitting API calls to the Global Database of Events, Language, and Tone (GDELT) database within R. The focus is on the Summary Explorer and TV News dataset. After reviewing several methods, I conclude that the Explorer GUI is the best method to construct the request URL. From this an export URL can be constructed from the GUI and a little bit of manual editing to change the format of the request from JSON to CSV.
author:
  - name: Nathan Craig
    url: https://ncraig.netlify.app/
date: 12-09-2021
categories:
  - how-to
output:
  distill::distill_article:
    self_contained: false
    # pandoc_args: ["--number-sections"]
    df_print: paged
    toc: true
    number_sections: false
    code_folding: true
toc: true
from: markdown+tex_math_single_backslash
link-citations: yes
repository_url: https://github.com/n8craig/ncraig
image: https://i.imgur.com/Ac0LYfZ.png
creative_commons: CC BY-NC
draft: FALSE
bibliography: references.bib
execute: 
  freeze: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The Global Database of Events, Language, and Tone ([GDELT](https://www.gdeltproject.org/)) is a big data project created by Dr. Kalev Leetaru and others. Among the many intriguing [tools](https://analysis.gdeltproject.org/) offered by GDELT the Television Database Explorer and API is noteworthy. Learning to use GDELT data can be a daunting experience, and I am definitely learning as I go. This page attempts to report some things I learned while digging into the Television Explorer that I believe others might find useful. A key purpose of this document is to review methods for directly accessing the API within R for use in R Markdown. First, I a brief mention of why an anthropologist might be interested in a TV News database. Then we will get into the "how of it", but feel free to jump ahead.

## Media: Imagined communities and moral panics

In May of 2021, as part of an exercise of thinking through and responding to an article by Adam Harris [-@harris2021a] I read in *The Atlantic Monthly*, I used GDELT along with the GDELT Global Online News Coverage and the Google Ngram Viewer to [discuss](https://ncraig.netlify.app/posts/2021-05-07-critical-race-theory/) a major uptick in news coverage of Critical Race Theory (CRT). GDELT allowed me to track TV coverage, the Global Online News Coverage gave a sense of tone, and the Google Ngram viewer gave a frequency chart with the longest temporal coverage. Intriguingly, Dr. Leetaru [@leetaru2021; @leetaru2021a] included Critical Race Theory in brief news summaries from June 6 and another from November 10 of 2021. Apparently, he noticed a spike in coverage of this topic too. The production of media is a process, but the intensity of news coverage of CRT seems to have emerged somewhat as an event in society [@sewell1996].

To have a better sense of this news event, I wanted to see when there was greater coverage over time and get a sense of the increased amplitude of coverage of CRT in 2021. I attempted to contextualize the various news coverage spikes to other contemporaneous processes: Dr. Bell's death, the nomination of Lani Guinier, Breitbart's publication of the Obama/Bell video, the murder of George Floyd, Chris Rufo's appearance on Fox News, Trump's signing EO 13950, and Biden's swearing in.

I cast my foray into GDELT data, as an application of the "culturomics" [@michel2011; @leetaru2011] approach which involves the distant reading [@schulz2011; @moretti2013] of large quantities of media. Print media is central to the creation of shared experiences that form the basis of "imagined communities" undergirding the nation state [@anderson2006a]. If print media is key to creating a shared experience, then television (which is a richer and more immerse type of media than print) should also play a similar role in fostering shared experiences and views in contemporary society.

In addition to creating imagined communities, media can also play an important role in the creation and spread of moral panics [@cohen2011; @cohen2011a; @moralpa2013]. In fact, media is a crucial ingredient in allowing moral panics to spread out of local contexts and can play a role in ranking the issue as among society's most pressing. One might think about the creation and spread of moral panics as miniature "imagined communities" of outraged believers that operate within larger national identities.

::: callout-note
Moral panic is manifest by: 1) collective action campaigns, 2) bills that criminalize the deviant behavior, 3) ranking of the issue high in the public's hierarchy, 4) public discussion of the issue in the media [@goode2009, 153].
:::

GDELT is intriguing to me as an anthropologist because it represents a way to examine large bodies of a type of complex media that permeates our lives. This stream of broadcast TV news contribute to mediascapes which foster particular ideoscapes [@appadurai1990, 296].

Using tools like GDELT or Google Ngram Viewer, we can examine these macro "scapes" of media and ideology to start to systematically track what as a society are we being presented, when, for how long,Â and at what intensity? I think there is quite a bit more to consider in regards to media coverage of CRT. However here in this "how to", I want to discuss some things I learned about retrieval of GDELT TV data while trying to understand TV news coverage of CRT.

## Application Programming Interfaces (API's) for reproducable and updateable research

API's are a good way to access data in the spirit of reproducible research [@implemen2014], particularly when the data are regularly updated like GDELT. In the past, I used the [Television Explorer](https://api.gdeltproject.org/api/v2/summary/summary?d=iatv&t=summary) interface, downloaded a CSV file, and worked with it on my hard drive. While I was grateful for the data, this method involved doing analysis on a static file. I would rather access the API directly, pull in the comma delimited data, and make a data frame for further processing all programmatically within an R Markdown document. Moreover, I want that request and data frame output to be able to run at some point again in the future--perhaps even updated programmatically.

I encountered three ways of communicating with the GDELT API inside R. Each has its own strengths and weaknesses. The three ways are:

-   `reader::read_csv` permits finer control over parameters but takes some setting up. Though it involves manually editing long URLs, **I find `read_csv` the preferred method of the three**.
-   `newsflash::query_tv` is the simplest and fastest method. If monthly resolution data are suitable this may be a turnkey solution for some applications.
-   `httr` and `jsonlite` will pull the data in JSON format with lots of ancillary information. I can also get back results from a CSV request. However, pulling out the desired product looks a bit daunting. Compared to `read_csv`, the `httr` and `jsonlite` method seem to involve several unnecessary steps.

Below I illustrate each of these three methods discussing the strengths and weaknesses. I focus on `read_csv()` because I believe it the best method for my purposes. Let's call our libraries and get into it.

::: callout-note
"Code chunks" (blocks of code) are folded by default. In these cases, clicking "Show Code" will reveal the underlying R commands. Some chunks are set to always display code, these are generally chunks illustrating a concept under discussion in the text.
:::

```{r load-libraries, message = FALSE, warning = FALSE}
library(tidyverse)
library(newsflash)
library(httr)
library(jsonlite)
library(ggplot2)
library(patchwork)
```

# CSV API request to `readr::read_csv()`

One of the GDELT 2.0 TV News API's six output [formats](https://blog.gdeltproject.org/gdelt-2-0-television-api-debuts/) is CSV. When an API request is made with `format=csv` the browser returns a CSV file. Therefore, a properly constructed GDELT TV News API request URL can be passed directly to `read_csv()` and the function will create a data frame. It is super simple, really that's it. Let's look at how to construct a request URL using the Explorer GUI and show how to set the output format to CSV. Then we'll be ready to drop that URL into a `read_csv()` call.

## Determining the API request parameters

GDELT 2.0 TV News API has a fair number of options; some really must be understood to proceed forward. The Explorer Summary search page is broken into a series of steps. I walk through these and discuss each in turn.

### Step 1: Dataset

The [Explorer](https://api.gdeltproject.org/api/v2/summary/summary?d=iatv&t=summary) GUI lets users choose among 5 different data sets:

-   **Global Online News Coverage**: Online textual news sources along with some images.
-   **Television News (Internet Archive)**: Full text search of complete raw closed captioning stream. See more [here](https://blog.gdeltproject.org/gdelt-2-0-television-api-debuts/).
-   **Television News AI (Internet Archive)**: Searches on visual entities, caption entities, automated speech recognition (ASR), caption keywords, and OCR of onscreen text. See more [here](https://blog.gdeltproject.org/television-2-0-ai-api-debuts/).
-   **Chyron Explorer (Internet Archive)**: Search the Internet Archive's "[Third Eye](https://archive.org/services/third-eye.php)" OCR stream.
-   **Chyron Browser**: Visual summaries of chyron text overlays for a specified date.

Once the desired data set is selected, the Explorer GUI refreshes and the search fields change. Though the API request structure is similar across the five data sets, each is slightly different because the data being queried are either from different news sources (written, TV closed captions, TV chyrons) or are the same data sources processed in different ways (TV closed captions vs. AI). Knowing which data set one is working with is a key step to successfully using the Explorer GUI and getting a properly structured request URL. Here I discuss parameters related to the TV News data set.

```{r search-box, echo=FALSE}
#| column: margin
knitr::include_graphics("https://i.imgur.com/L7r3FwI.png")
```

To the far right, in the section where it says "Enter Search" there is an blue "i" info button. Clicking this reveals hidden text that explains more about the data structure and how to construct searches. Each parameter has its own info button. Reading these entries carefully and consulting them frequently is useful for grasping the details of the search process.

::: callout-note
The "i" buttons may not work properly in Firefox. They work in Chrome.
:::

### Step 2: Output Type

One must choose to display results of a single query as a "Summery Overview Dashboard" or a comparison of multiple queries as a "Comparison Visualization." Both types of output are useful and work in similar ways. Here, I discuss single query "Summery Overview Dashboard" results.

### Step 3: Enter Search

Many big decisions are made in this section including the search term and several important parameters. Below are some thoughts on the search syntax and key parameters for the TV News data set:

-   **Keyword**: This is the most complex field and is where the search term is constructed. Be sure to read the blue info button. There is also a tutorial [video](https://www.youtube.com/watch?v=PH-5BV-xwpc&t=3s) that walks through some more complex search examples. Below are some search syntax rules:

    -   Only exact keywords are matched. Eg. a search on `russia` **will not** return results for "russian" or "russians".
    -   Groups of OR'd terms can be enclosed in parentheses. Eg. `(russia OR russian OR russians)` searches all three terms.
    -   Phrases are enclosed in quotes and may be as long as five terms. Eg. `"critical race theory"`
    -   Place a `-` in front of terms or phrases to exclude these from searches. Eg. `education -"critical race theory"` returns clips containing education that do not also mention "critical race theory".
    -   Search terms are not translated. Therefore, words are language specific.
    -   It is possible to search for terms that might be in adjacent 15 second clips using the context operator. Eg. `clinton (context:"email" OR context:"emails" OR context:"server")` returns all clips that contain clinton and which also contain either "email," "emails," or "server" either in the same or adjacent clips.

-   **Time Period**: I generally request the entire time period. It can be filtered later in R.

-   **Stations**: This defaults to CNN, Fox, and MSNBC. For US news, I select all of the stations listed under `Active National`. There is also an optional `market` parameter that one can use, but calling it involves manually deleting the stations and replacing them with `market=national`. I am not aware of a way to do this via the GUI.

-   **Combine/Separate**: One must choose whether values are Separate Stations with Multiple Timelines or Combine Stations for a Single Timeline. I am often interested in comparing values between stations and thus select Separate Stations for Multiple Timelines. If one is interested in the total volume of coverage on a given topic, then select Combine Stations to generate a Single Timeline. One can even envision a scenario where the same term might be searched both combined and single to see what portion of total coverage each individual station is responsible for.

-   **Normalized**: If the plan is to make comparisons between stations or search terms, the data should be normalized.

-   **Date Resolution**: Here one can set the units of time for which data are reported. If Time Period \<7 days the Date Resolution defaults to hourly. If Time Period \<3 years the Default Date resolution is daily, and when Time Period \>3 years the default Date Resolution is monthly. These defaults can be overridden. I generally request daily resolution.

-   **Time Zone**: The default is UTC. I personally do not change this, and believe it only has relevance when working with hourly Date Resolution data.

-   **Smoothing**: The default is No Smoothing. I personally do not change this.

### Step 4: Displays

Since the aim here is to obtain a data frame, the display parameters might not seem that important. However, by turning these settings on one can get a quick check on the search and also have a fuller sense of what was returned. For example, it can be informative to watch a couple of clips or review some of the visualizations. I recommend including zoomable timelines and including all displays possible. Note that for me the Google Trends Timeline does not function so I often turn it off.

### Getting the link {#getting-the-link}

Once search parameters are determined it is time to generate an export URL. Below are the steps I use to get a request URL to pass to `read_csv`:

-   Using the GDELT Summary [Explorer](https://api.gdeltproject.org/api/v2/summary/summary?) GUI set the search terms and execute a search. This returns a web page in a new browser tab. Eg. Here is the search [summary](https://api.gdeltproject.org/api/v2/summary/summary?d=iatv&t=summary&k=%22critical+race+theory%22&ts=full&fs=station%3ACNN&fs=station%3AFOXNEWS&fs=station%3AMSNBC&svt=zoom&svts=zoom&swvt=zoom&ssc=yes&sshc=yes&swc=yes&stcl=yes&c=1) for "critical race theory".
-   Check the results page noting the "Data Source" and "Human Summary" fields to confirm that the desired search was submitted. If the search is not correct, modify and resubmit.

```{r search-details, echo=FALSE}
knitr::include_graphics("https://i.imgur.com/7kglrgi.png")
```

-   If the desired search was submitted, then copy the URL to a text document for future use. *This is the permanent URL for this particular search and should be used when communicating results.* Dr. Leetaru uses links to the summary page as a reference when writing on the web. I follow that convention.
-   In the search results that are returned in the new page, find the "Export" hamburger menu and select "Save data as JSON". This selection also returns a new web page in a new tab.

```{r export-interface, echo=FALSE}
knitr::include_graphics("https://i.imgur.com/rhgtoa1.png")
```

-   In that new tab, copy the URL and paste it into a text document (like an R Markdown file).
-   Change the format variable of the URL from `format=json` to `format=csv`.
-   Once the format parameter is changed, paste the modified URL into a `read_csv()` call. This will return the desired CSV file, which is read with `read_csv()`, and can be assigned to a variable like `df`.
-   It is also useful to copy the exact search terms that were used and assign these to a variable to be included in reports or graphs.

The examples below use an export URL with read_csv to create a data frame which is then passed to ggplot.

::: callout-note
In the code chunks, please forgive the long API URLs that neither scroll nor wrap. This is a persistent [issue](https://stackoverflow.com/questions/33481271/how-to-wrap-code-and-the-output-in-markdown-rmd) for which I found no clear resolution shy of breaking up the URL which is obviously undesirable. The cleanest method I envisioned to make the URL available is to put it in a link. Request URL's are supplied in a link to the right of the data frame.
:::

::: callout-note
In the code chunk below, please notice the use of double quotes in the `search_term` variable. In R, [strings](https://r4ds.had.co.nz/strings.html#string-basics) are created under either single or double quotes, there is no difference. If the string contains `"` then in R it is generally recommended to use `'` for the outer quotes. I follow this convention. In doing this while working with the Explorer GUI, it is possible to copy the search directly from the box and place that whole term in single quotes.
:::

```{r search-results-list, echo=FALSE}
#| column: margin

knitr::include_graphics("https://i.imgur.com/DhhDMpL.png")
knitr::include_graphics("https://i.imgur.com/2JwtvdN.png")
```

```{r read-csv-simple-call, message = FALSE, warning = FALSE, code_folding=FALSE}
# paste in the text from the GUI search box.
# note that entire string should be contained in single quotes
search_term <- '"critical race theory"'

# Paste the request URL into a read_csv call and assign to variable
df <- read_csv("https://api.gdeltproject.org/api/v2/tv/tv?format=csv&timespan=FULL&last24=yes&dateres=DAY&query=%22critical%20race%20theory%22%20(station:BLOOMBERG%20OR%20station:CNBC%20OR%20station:CNN%20OR%20station:CSPAN%20OR%20station:CSPAN2%20OR%20station:CSPAN3%20OR%20station:FBC%20OR%20station:FOXNEWS%20OR%20station:MSNBC%20)%20&mode=timelinevol&timezoom=yes")  %>% 
  rename(date = 1,
         network = 2,
         value = 3) %>% 
  mutate(search = search_term)
df
```

Copy [request URL](https://api.gdeltproject.org/api/v2/tv/tv?format=csv&timespan=FULL&last24=yes&dateres=DAY&query=%22critical%20race%20theory%22%20(station:BLOOMBERG%20OR%20station:CNBC%20OR%20station:CNN%20OR%20station:CSPAN%20OR%20station:CSPAN2%20OR%20station:CSPAN3%20OR%20station:FBC%20OR%20station:FOXNEWS%20OR%20station:MSNBC%20)%20&mode=timelinevol&timezoom=yes).

```{r read-csv-simple-plot, fig.cap="Sample output timeline of a simple search retrieved using `read_csv`."}
df %>% 
  ggplot(aes(date, value, color=network))+
  geom_line(alpha = 0.75)+
  labs(title = paste("TV Volume Timeline:", search_term),
     subtitle = "Coverage Volume Normalized Daily Resolution",
     caption = "Data Source: https://api.gdeltproject.org/api/v2/summary/summary",
     color = "Network")+
xlab("Date")+
ylab("% Airtime (15sec Blocks)")

```

### Complex Searches

Below is an example illustrating a complex multi-term search on Critical Race Theory looking only at contexts where "school", "education", "classroom", and "school board" were not mentioned. One can see that the overall structure is similar to the prior figure but the amplitude of the highest peak is lower once some education related terms are used to constrain the search. This change in amplitude illustrates that a portion of the conversation around CRT is related to education.

```{r read-csv-complex-call, message = FALSE, warning=FALSE, code_folding=FALSE, echo=FALSE}
# the /n is added for a carriage return in the figure.
search_term <- '"critical race theory" -context:"school"\n -context:"education"-context:"classroom" -context:"school board"'

df <- read_csv("https://api.gdeltproject.org/api/v2/tv/tv?format=csv&timespan=FULL&last24=yes&dateres=DAY&query=%22critical%20race%20theory%22%20-context:%22school%22%20-context:%22education%22%20-context:%22classroom%22%20-context:%22school%20board%22%20(station:BLOOMBERG%20OR%20station:CNBC%20OR%20station:CNN%20OR%20station:CSPAN%20OR%20station:CSPAN2%20OR%20station:CSPAN3%20OR%20station:FBC%20OR%20station:FOXNEWS%20OR%20station:MSNBC%20)%20&mode=timelinevol&timezoom=yes")  %>% 
  rename(date = 1,
         network = 2,
         value = 3) %>% 
  mutate(search = search_term)
```

<aside>Copy [request URL](https://api.gdeltproject.org/api/v2/tv/tv?format=csv&timespan=FULL&last24=yes&dateres=DAY&query=%22critical%20race%20theory%22%20-context:%22school%22%20-context:%22education%22%20-context:%22classroom%22%20-context:%22school%20board%22%20(station:BLOOMBERG%20OR%20station:CNBC%20OR%20station:CNN%20OR%20station:CSPAN%20OR%20station:CSPAN2%20OR%20station:CSPAN3%20OR%20station:FBC%20OR%20station:FOXNEWS%20OR%20station:MSNBC%20)%20&mode=timelinevol&timezoom=yes).</aside>

```{r read-csv-complex-plot, fig.cap="Sample output timeline of a complex search retrieved using `read_csv`.", preview=TRUE}
df %>% 
  ggplot(aes(date, value, color=network))+
  geom_line(alpha = 0.75)+
  labs(title = paste("TV Volume Timeline:", search_term),
     subtitle = "Coverage Volume Normalized Daily Resolution",
     caption = "Data Source: https://api.gdeltproject.org/api/v2/summary/summary",
     color = "Network")+
xlab("Date")+
ylab("% Airtime (15sec Blocks)")
```

### Search Comparisons

If at Step 3 one selects "Comparison Visualization" it is possible to compare multiple search results. To serve as an example, it is possible to plot mentions of "Critical Race Theory" in comparison to a search regarding the combined mentions of "evolution", "evolutionary theory", or "natural selection". What one finds is that at the peak of CRT coverage it was mentioned more frequently than all other terms combined. Unfortunately, at this time the Summary Explorer GUI does not have an option to export JSON and I'm not presently sure how to construct the request URL.

<aside>It is worth noting that equivalent data comparing searches can be produced by output from two Summery Overview Dashboards with each set to Combine Stations for a Single Timeline.</aside>

```{=html}
<style>
.respviz { height: 400px; }
@media screen and (min-width : 0px) and (max-width : 767px){ .respviz { height: 250px; } }
</style>
<iframe src="https://api.gdeltproject.org/api/v2/summary/summary?d=iatv&t=compare&format=csv&k1=%22critical+race+theory%22&fs1=station%3ABLOOMBERG&fs1=station%3ACNBC&fs1=station%3ACNN&fs1=station%3ACSPAN&fs1=station%3ACSPAN2&fs1=station%3ACSPAN3&fs1=station%3AFBC&fs1=station%3AFOXNEWS&fs1=station%3AMSNBC&k2=%28evolution+OR+evolutionary+OR+%22natural+selection%22+OR+genetic+OR+genetics+OR+genomics+OR+genome%29&fs2=station%3ABLOOMBERG&fs2=station%3ACNBC&fs2=station%3ACNN&fs2=station%3ACSPAN&fs2=station%3ACSPAN2&fs2=station%3ACSPAN3&fs2=station%3AFBC&fs2=station%3AFOXNEWS&fs2=station%3AMSNBC&fs3=station%3ACNN&fs3=station%3AFOXNEWS&fs3=station%3AMSNBC&fs4=station%3ACNN&fs4=station%3AFOXNEWS&fs4=station%3AMSNBC&fcs=comb&stm=yes&c=1&cvt=embed&ct=timelinemerge" scrolling="no" width=100% frameborder="0" class="respviz"></iframe>
```
### Search Comparison Fiddling (NOT PUBLISHED)

For any search, like "[critical race theory](https://api.gdeltproject.org/api/v2/summary/summary?d=iatv&t=summary&k=%22critical+race+theory%22&ts=full&fs=station%3ABLOOMBERG&fs=station%3ACNBC&fs=station%3ACNN&fs=station%3ACSPAN&fs=station%3ACSPAN2&fs=station%3ACSPAN3&fs=station%3AFBC&fs=station%3AFOXNEWS&fs=station%3AMSNBC&fdr=day&svt=zoom&svts=zoom&swvt=zoom&ssc=yes&sshc=yes&swc=yes&stcl=yes&c=1)" one can return a wordcloud and list of most common terms from the 200 most relevant matching clips. At the bottom right of the wordcloud, there is a direct link to a CSV of the common terms. One can see that after removing the search terms (i.e. "Race", "Theory", and "Critical") four of the top five words relate to an educational setting: "School", "Schools", "Taught", and "Parents". Reviewing the 100 most frequent words, I made a list of terms related to the educational situation. We can call this a *theme dictionary* because it is a dictionary of terms related to our theme, which in this case is education. The theme dictionary can be used to perform a NOT filter on the main search term thereby returning clips where none of the theme dictionary terms are mentioned. Comparing the two provides a way to track the portion of coverage of a given query as it relates to a specific theme.

```{r top-words}
search_term <- '"critical race theory"'
df <- read_csv("https://api.gdeltproject.org/api/v2/tv/tv?format=csv&timespan=FULL&last24=yes&dateres=DAY&query=%22critical%20race%20theory%22%20(station:BLOOMBERG%20OR%20station:CNBC%20OR%20station:CNN%20OR%20station:CSPAN%20OR%20station:CSPAN2%20OR%20station:CSPAN3%20OR%20station:FBC%20OR%20station:FOXNEWS%20OR%20station:MSNBC%20)%20&mode=wordcloud")
df
```

Search with theme dictionary filtered out of the query "[Critical Race Theory](https://api.gdeltproject.org/api/v2/summary/summary?d=iatv&t=summary&k=%22Critical+Race+Theory%22+-School+-Schools+-Taught+-Parents+-Teach+-Teaching+-Teachers+-Children+-Education+-Kids+-Students+-Professor+-Curriculum&ts=full&fs=station%3ABLOOMBERG&fs=station%3ACNBC&fs=station%3ACNN&fs=station%3ACSPAN&fs=station%3ACSPAN2&fs=station%3ACSPAN3&fs=station%3AFBC&fs=station%3AFOXNEWS&fs=station%3AMSNBC&svt=zoom&svts=zoom&swvt=zoom&ssc=yes&sshc=yes&swc=yes&stcl=yes&c=1)".

```{r}
search_term1 <- '"critical race theory"'
df_1 <- read_csv("https://api.gdeltproject.org/api/v2/tv/tv?format=csv&timespan=FULL&last24=yes&query=%22Critical%20Race%20Theory%22%20(station:BLOOMBERG%20OR%20station:CNBC%20OR%20station:CNN%20OR%20station:CSPAN%20OR%20station:CSPAN2%20OR%20station:CSPAN3%20OR%20station:FBC%20OR%20station:FOXNEWS%20OR%20station:MSNBC%20)%20&mode=timelinevol&timezoom=yes")  %>% 
  rename(date = 1,
         network = 2,
         value = 3) %>% 
  mutate(search = search_term)
df_1

search_term2 <- '"Critical Race Theory" -School -Schools -Taught -Parents -Teach -Teaching -Teachers -Children -Education -Kids -Students -Professor -Curriculum'
df_2 <- read_csv("https://api.gdeltproject.org/api/v2/tv/tv?format=csv&timespan=FULL&last24=yes&query=%22Critical%20Race%20Theory%22%20-School%20-Schools%20-Taught%20-Parents%20-Teach%20-Teaching%20-Teachers%20-Children%20-Education%20-Kids%20-Students%20-Professor%20-Curriculum%20(station:BLOOMBERG%20OR%20station:CNBC%20OR%20station:CNN%20OR%20station:CSPAN%20OR%20station:CSPAN2%20OR%20station:CSPAN3%20OR%20station:FBC%20OR%20station:FOXNEWS%20OR%20station:MSNBC%20)%20&mode=timelinevol&timezoom=yes")  %>% 
  rename(date = 1,
         network = 2,
         value = 3) %>% 
  mutate(search = search_term)
df_2

# df <- df_crt %>% inner_join(df_edu, by = c("date", "network")) %>% 
#   rename(CRT = value.x) %>% 
#   rename(EDU = value.y) %>% 
#   select(c(-search.x, -search.y)) %>% 
#   pivot_longer(cols = c(CRT, EDU), names_to = "search", values_to="value")

```

```{r}
df_1 %>% 
  ggplot(aes(date, value, color=network))+
  geom_line(alpha = 0.75)+
  labs(title = paste("TV Volume Timeline:", search_term1),
     subtitle = "Coverage Volume Normalized Daily Resolution",
     caption = "Data Source: https://api.gdeltproject.org/api/v2/summary/summary",
     color = "Network")+
xlab("Date")+
ylab("% Airtime (15sec Blocks)")


```

# API Request with `newsflash::query_tv`

The `query_tv` function of the `newsflash` library provides a quick and easy way to access the Television Archive data set. I would like to learn more about building wrappers of this kind. While I'm very impressed by `query_tv` there are some matters to be aware of when using it.

## Monthly Resolution

The `query_tv` function returns monthly results for the full time range of the data set. This is suitable resolution for many purposes. However, if one wants daily, or finer, resolution data then another method of submitting requests is required because `query_tv` does not seem to have arguments for date resolution.

## Single Search Term

The `query_tv` only permits searching on a single term. This is equivalent to data displayed in and exportable from the Summary Overview Dashboard. Within `query_tv` I don't see options for Comparison Visualization. If searches on a single query are all that is required `query_tv` may be appropriate, but it can't handle query comparisons. However, at this point, I'm not sure how to construct a request URL for a Comparison Visualization and would need to build comparisons by merging individual queries. Therefore, `query_tv`'s single search limitation is pretty trivial.

```{r query-tv-simple-call, warning = FALSE, code_folding=FALSE}
# search_term <- params$search
search_term <- '"critical race theory"'

starting_date <- NULL

df <- query_tv(search_term,
               mode = "TimelineVol",
               start_date = starting_date) %>% 
  tibble() %>% 
  arrange(date) %>% 
  # filter(network %in% c("CNBC","CNN", "CSPAN", "FOX News", "MSNBC")) %>% 
  mutate(network = as.factor(network),
         search = search_term)
df
```

```{r query-tv-simple-plot, fig.cap="Sample results generated from `query_tv`."}
df %>% 
  ggplot(aes(date, value, color=network))+
  geom_line(alpha = 0.75)+
  labs(title = paste("TV Volume Timeline:", search_term),
     subtitle = "Coverage Volume Normalized Monthly Resolution",
     caption = "Data Source: https://api.gdeltproject.org/api/v2/summary/summary",
     color = "Network")+
xlab("Date")+
ylab("% Airtime (15sec Blocks)")
```

## Watch the Quotation Marks

In the GDELT interface, if one wants to submit a multi-word search using `AND` operators the user simply enters a space delimited list of words (see note above). This returns a search composed of a series of words joined by `AND` Boolean operators. If one wants to search on an exact phrase, it is placed within quotes. It is possible to string together multiple exact phrases separated by spaces for an `AND` operator. It is super simple, but when working in R there is an issue to consider.

In R, when defining strings that need to contain quotation marks (`""`), which is the case with exact phrase searches of GDELT, it is important to use double quotes (i.e. `'""'`). Doing so will achieve the results of single quotes in the Explorer Search GUI. When working with `query_tv`, this very basic fact of R initially confused me and led to some unexpected results. I figured out the issue by generating some comparison graphs. **In R, string variables must always be placed in quotes. Therefore, to get a string that contains quotes, one must use double quotes.** This would be true regardless of what method one is using to communicate with an API.

```{r query-tv-comparison-searches, code_folding=FALSE}
# search terms used to make following graph
# note how the term is constructed noting the result
search_term1 <- 'critical race theory'
search_term2 <- '"critical race theory"'
search_term3 <- 'theory race critical'
search_term4 <- '"theory race critical"'
search_term5 <- '(critical OR race OR theory)'
search_term6 <- '"(critical OR race OR theory)"'
```

```{r query-tv-comparison-plot, fig.height = 8, fig.cap="Plot comparing the output of different search terms."}
df1 <- query_tv(search_term1)
df2 <- query_tv(search_term2)
df3 <- query_tv(search_term3)
df4 <- query_tv(search_term4)
df5 <- query_tv(search_term5)
df6 <- query_tv(search_term6)

p1 <- df1 %>% 
  ggplot(aes(date, value, color=network))+
  geom_line(alpha = 0.75)+
  labs(title = search_term1,
     color = "Network")+
  xlab("Date")+
  ylab("% Airtime (15sec Blocks)")+
  theme(legend.position = "none")

p2 <- df2 %>% 
  ggplot(aes(date, value, color=network))+
  geom_line(alpha = 0.75)+
  labs(title = search_term2,
     color = "Network")+
  xlab("Date")+
  ylab("% Airtime (15sec Blocks)")+
  theme(legend.position = "none")

p3 <- df3 %>% 
  ggplot(aes(date, value, color=network))+
  geom_line(alpha = 0.75)+
  labs(title = search_term3,
     color = "Network")+
  xlab("Date")+
  ylab("% Airtime (15sec Blocks)")+
  theme(legend.position = "none")

p4 <- df4 %>% 
  ggplot(aes(date, value, color=network))+
  geom_line(alpha = 0.75)+
  labs(title = search_term4,
     color = "Network")+
  xlab("Date")+
  ylab("% Airtime (15sec Blocks)")+
  theme(legend.position = "none")

p5 <- df5 %>% 
  ggplot(aes(date, value, color=network))+
  geom_line(alpha = 0.75)+
  labs(title = search_term5,
     color = "Network")+
  xlab("Date")+
  ylab("% Airtime (15sec Blocks)")+
  theme(legend.position = "none")

p6 <- df6 %>% 
  ggplot(aes(date, value, color=network))+
  geom_line(alpha = 0.75)+
  labs(title = search_term6,
     color = "Network")+
  xlab("Date")+
  ylab("% Airtime (15sec Blocks)")+
  theme(legend.position = "none")

(p1+p2)/(p3+p4)/(p5+p6) & plot_layout(guides = "collect")

```

# API Requests with `httr`

When launching out to try to understand how to access API's via R, I quickly encountered pages that discussed the `httr` library. It seems to be a very common way of accessing API's via R, and thus worth some mention. The library's "Getting started with `httr`" [page](https://httr.r-lib.org/articles/quickstart.html) is the best place to start understanding how to use it. An R-bloggers [post](https://www.r-bloggers.com/2015/02/using-the-httr-package-to-retrieve-data-from-apis-in-r/) on "using the httr package to retrieve data from apis in R" is also very helpful.

I started out using `httr` before reasoning that I could simply use `read_csv`. There are some working `httr` examples below, I may be going about this wrong but most of them still end up relying on `read_csv`.

## Request to GDELT using `httr`: table embedded in JSON

Using the `GET()` method it was possible to issue a request to GEDELT 2.0 TV and receive JSON data. If `format` is set to `csv` it will return CSV. To access the data, that response just goes back to `read_csv`. In this case, it is easier to simply pass the request URL directly to `read_csv` and get back a data frame.

```{r code_folding=FALSE}
res <- GET("https://api.gdeltproject.org/api/v2/tv/tv?format=csv&timespan=FULL&last24=yes&dateres=DAY&query=%22critical%20race%20theory%22%20(station:BLOOMBERG%20OR%20station:CNBC%20OR%20station:CNN%20OR%20station:CSPAN%20OR%20station:CSPAN2%20OR%20station:CSPAN3%20OR%20station:FBC%20OR%20station:FOXNEWS%20OR%20station:MSNBC%20)%20&mode=timelinevol&timezoom=yes")
data <- rawToChar(res$content)
read_csv(data)
```

<aside>Copy [request URL](https://api.gdeltproject.org/api/v2/tv/tv?format=csv&timespan=FULL&last24=yes&dateres=DAY&query=%22critical%20race%20theory%22%20(station:BLOOMBERG%20OR%20station:CNBC%20OR%20station:CNN%20OR%20station:CSPAN%20OR%20station:CSPAN2%20OR%20station:CSPAN3%20OR%20station:FBC%20OR%20station:FOXNEWS%20OR%20station:MSNBC%20)%20&mode=timelinevol&timezoom=yes).</aside>

If the request URL is set to `format=json` the data are returned but in nested json. The data of interest are broken into components and buried in a nested list. Assembling the desired table from this result would probably require `lapply` or `sapply` and `bind_rows`. I would need to brush up on how to do this. However, the entities can be inspected with `pluck`. It would be good in principle to know how to reassemble the desired table from the JSON data, but in this context it seems unnecessary because there are simpler methods to obtain the data table.

```{r code_folding=FALSE}
res <- GET("https://api.gdeltproject.org/api/v2/tv/tv?format=json&timespan=FULL&last24=yes&dateres=DAY&query=%22critical%20race%20theory%22%20(station:BLOOMBERG%20OR%20station:CNBC%20OR%20station:CNN%20OR%20station:CSPAN%20OR%20station:CSPAN2%20OR%20station:CSPAN3%20OR%20station:FBC%20OR%20station:FOXNEWS%20OR%20station:MSNBC%20)%20&mode=timelinevol&timezoom=yes")

data = fromJSON(rawToChar(res$content))
data
```

<aside>Copy [request URL](https://api.gdeltproject.org/api/v2/tv/tv?format=json&timespan=FULL&last24=yes&dateres=DAY&query=%22critical%20race%20theory%22%20(station:BLOOMBERG%20OR%20station:CNBC%20OR%20station:CNN%20OR%20station:CSPAN%20OR%20station:CSPAN2%20OR%20station:CSPAN3%20OR%20station:FBC%20OR%20station:FOXNEWS%20OR%20station:MSNBC%20)%20&mode=timelinevol&timezoom=yes).</aside>

```{r}
data %>% pluck(2) %>% pluck(1)
```

## Further Fiddling

Given an API's documentation, it is possible to manually build a request URL based on a set of input parameters. That request can then be passed to `read_csv` or `GET`. GDELT has a nice GUI interface that lets users know what data set they searched and the details of the search term. The Explorer Summary URL returned by the GUI is a really nice general reference to the search from which a request URL can be created ([example](https://api.gdeltproject.org/api/v2/summary/summary?d=iatv&t=summary&k=%22critical+race+theory%22&ts=full&fs=station%3ABLOOMBERG&fs=station%3ACNBC&fs=station%3ACNN&fs=station%3ACSPAN&fs=station%3ACSPAN2&fs=station%3ACSPAN3&fs=station%3AFBC&fs=station%3AFOXNEWS&fs=station%3AMSNBC&fdr=day&svt=zoom&svts=zoom&svhm=basic&swvt=zoom&ssc=yes&sshc=yes&swc=yes&stcl=yes&c=1)). However, not all API's have handy GUI's for building request URLs or have output dashboards. In fact, many do not. Thus, knowing how to build a request URL can be a useful skill.

Since GDELT has quite a few parameters, and some of them are a bit complicated, learning to build a request URL is a good exercise. I used a combination of `URLencode` to deal with spaces and quotation marks along with `paste` to join the various parts together. Most parameters are joined by `&`. However, between the `query` term and `station` information there is no `&` rather it is a `%20` or space. So this part of the request URL needs to be built slightly differently.

```{r warning=FALSE, code_folding=FALSE}
# Set parameters
url <- "https://api.gdeltproject.org/api/v2/tv/tv?"

# query <- c('"critical race theory"', "-school")
query <- '"critical race theory" -school'
market <- '"National"'
mode <- "timelinevol"
format <- "csv"
datanorm <- "perc"
dateres <- "day"
timelinesmooth <- 0
datacomb <- "sep"
last24 <- "yes"
timezoom <- "yes"

# Build a query string from a vector
query <- URLencode(query)
query <- paste("query=", query, sep = "")
market <- URLencode(market)
market <- paste("%20market:", market, sep = "")
mode <- paste("mode=", mode, sep = "")
format <- paste("format=", format, sep = "")
datanorm <- paste("datanorm=", datanorm, sep="")
dateres <- paste("dateres=", dateres, sep = "")
timelinesmooth <- paste("timelinesmooth=", timelinesmooth, sep = "")
datacomb <- paste("datacomb=", datacomb, sep = "")
last24 <- paste("last24=", last24, sep = "")
timezoom <- paste("timezoom=", timezoom, sep = "")

# Build request URL
request <- paste(url, query, market, sep = "")
request <- paste(request, mode, format, datanorm, dateres, timelinesmooth, datacomb, last24, timezoom, sep = "&")
```

```{r, code_folding=FALSE}
# Make GET request
res <- GET(request)
data = rawToChar(res$content)
read_csv(data)
```

That seems like a lot of work to get the data when the `request` variable can just be passed to `read_csv` and return a data frame. Still, I believe it is a good exercise to figure out how to construct a request URL from a set of parameters.

```{r, code_folding=FALSE}
read_csv(request)
```
